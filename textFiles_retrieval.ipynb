{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0GmpzevMPFT6",
        "outputId": "ad45ded4-0121-404d-b7b9-f9b9e201cdb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u442-b06~us1-0ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive2\n",
        "\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlWciRMuNiUb",
        "outputId": "671e48d5-6997-4e42-cdeb-d4b7bcb8f916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTcFt7UMPDiq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.rdd import RDD\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B2Yg9wFP9ci"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"VSM Retrieval\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT9BK6djQB4p",
        "outputId": "5ae69493-ed6c-41ad-c4c3-23d2762044e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of documents:\n",
            "1460\n"
          ]
        }
      ],
      "source": [
        "# Define path to input text files (change this to your directory)\n",
        "input_dir = \"/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/\"\n",
        "\n",
        "# Load all text files into an RDD\n",
        "text_rdd = sc.wholeTextFiles(input_dir)  # Each element is (filename, content)\n",
        "print(\"Total number of documents:\")\n",
        "print(text_rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ibPHKq8w5h9I",
        "outputId": "469e5fd2-5657-4563-a640-75e605233377"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('file:/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/941.txt',\n",
              "  'ISBD Standard or Secret?\\n  The controversial ISBD will mean radical changes in descriptive cataloging practice if put into operation, as planned, by the Library of Congress. Users of LC catalog cards will require retraining: all reference librarians  will experience an immediate and continuing demand for explanation of the  new catalog cards to users; those large public and research libraries with  computer-based systems will require costly modifications of computer programs. Yet the ISBD (International Standard Bibliographic Description) is destined  to be implemented by the Library of Congress with the sanction of ALA but  without even being considered by the recognized standards associations (ISO,  ANSI), to say nothing of the other professional and information industry  organizations.\\n'),\n",
              " ('file:/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/1014.txt',\n",
              "  'The Microform Revolution\\n            Librarians have tried replacing some of their books and journal files with microfilm copies or other microforms in order to save valuable space in the bookstacks, instead of or in addition to extension of the stack area, decentralization, compact shelving, separate storage warehouse, or any of the other solutions to the storage problem discussed in earlier chapters.  As a final paper, this solution for the storage of library materials will be discussed.  Although the distinct forms will not often be designated, \"microform\" is used here to mean the four forms most common in the United States: 35 mm. roll microfilm, microfiche (now standardized in the United States and Great Britain at 4 by 6 inches) and the two  micro-opaque forms - 3 by 5 inch Microcard, and 6 by 9 inch Microprint.\\n'),\n",
              " ('file:/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/935.txt',\n",
              "  \"Crisis in Library Education\\n  A battle is currently being waged at the University of Maryland School of Library and Information Services; its outcome will influence the future of the library profession.  Because of the significance of this controversy, this situation at Maryland is here brought to the attention of the profession at large.   Last fall, a group, with funding from the Office of Education, brought into being an educational program which is addressing the profession's most critical problems, those central to its survival as a viable social institution:  The library profession's need to define and fulfill an important service function in the public arena; its failure to make itself relevant to other than middle-class interests in the culture; and its inavailability to contribute to the alleviation of the severe social, economic, and other inequalities which exist  in the culture and which continue to deprive black Americans particularly of  even minimal life opportunities.\\n\"),\n",
              " ('file:/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/1002.txt',\n",
              "  'AACR 6: Time for a Review\\n   Two changes are proposed in the North American text of rule 6 of the Anglo-American Cataloging Rules (AACR): the adoption of the British text of 6B and the deletion of 6C.. Both of these changes are intended to simplify the entry of serials.. With the deletion of 6C, serials would be entered only under  title or corporate author.. The adoption of the British text od 6B would in turn greatly simplify the remaining choice between title or corporate author..\\n'),\n",
              " ('file:/content/drive/MyDrive/EECS4415_BigDataSystem/A2/CISI-IndividualDocuments/1006.txt',\n",
              "  'Library Service to the Disadvantaged\\n        This volume is intended mainly as a source book for project and program ideas for libraries now engaged in working with the disadvantaged or planning to do so.  It is also hoped that it will give an overview of the progress to date in work with the disadvantaged by bringing together much of the thinking and many of the ideas that have appeared in literature or in conferences in the last few years.\\n')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See the top 5 elements of text_rdd\n",
        "text_rdd.take(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yik-JcN89EJ8"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8D-jOWU0FJj"
      },
      "source": [
        "**Read a list of stopwords from a file named stop_words.lst (which is a text file with one stopword per line)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d9UZ4_vrsm",
        "outputId": "b7bb8cca-44d9-4fd8-9e67-71aff906f323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of stopword load: 319\n"
          ]
        }
      ],
      "source": [
        "# Read the stopwords from the stop_words.txt file and put them in a set named stop_words\n",
        "stop_words=set()\n",
        "#my google drive path to stopword\n",
        "stopwords_path=\"/content/drive/MyDrive/EECS4415_BigDataSystem/A2/stop_words.txt\"\n",
        "\n",
        "with open(stopwords_path,'r')as fil:\n",
        "  for line in fil:\n",
        "    #remove line breaks\n",
        "    word=line.strip()\n",
        "    stop_words.add(word)\n",
        "\n",
        "print(f\"# of stopword load: {len(stop_words)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JgBT1LCs5i3"
      },
      "source": [
        "**Define a function to get the filename prefix without the full path name and the file extension. The prefix will be used as the document ID.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g47HsyCnsvAC"
      },
      "outputs": [],
      "source": [
        "def get_filename_prefix(filepath):\n",
        "  try:\n",
        "    filename = os.path.basename(filepath)  # Get the filename from the path\n",
        "    filename_without_ext = os.path.splitext(filename)[0]  # Remove the extension\n",
        "    return filename_without_ext\n",
        "  except:\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GvagObEQ71q"
      },
      "outputs": [],
      "source": [
        "# Use the Porter Stemmer from NLTK\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(document):\n",
        "  filename, content = document\n",
        "  words = re.findall(r\"\\b[a-z]+\\b\", content.lower())  # Tokenization\n",
        "  #remove stopwords\n",
        "  filtered_word=[word for word in words if word not in stop_words]\n",
        "\n",
        "  #stemming\n",
        "  filtered_words=[stemmer.stem(word) for word in filtered_word]\n",
        "\n",
        "\n",
        "  return (get_filename_prefix(filename), filtered_words)  # (doc_id, [word1, word2, word3, ...])\n",
        "\n",
        "  #ps. i really should consider use other way name those valueable\n",
        "  #it casued lots of time find out i wasn't use the stemmed word........................\n",
        "\n",
        "# Convert text_rdd into another RDD (doc_words_rdd) whose elment is (doc_id, word_list)\n",
        "doc_words_rdd = text_rdd.map(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iViKZvj4-PgU"
      },
      "source": [
        "Build Inverted Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_xTV9TyBZ3D"
      },
      "source": [
        "**Compute term frequency (tf_value) of each word in each document, and put the results in a RDD (tf_rdd) where each element is (word, (doc_id, tf_value)). The relative term frequency value should be used, that is, the tf_value of a word $w$ in a document $d$ should be the number of occurrences of $w$ in $d$ divided by the total number of words in $d$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATDC4PRHRHPj"
      },
      "outputs": [],
      "source": [
        "# Compute Term Frequency (TF)\n",
        "def compute_tf(doc_tuple):\n",
        "    doc_id,words=doc_tuple\n",
        "    #total # of words inside doc\n",
        "    total_words=len(words)\n",
        "\n",
        "    #count each words occured time\n",
        "    word_counts={}\n",
        "    for w in words:\n",
        "      if w not in word_counts:\n",
        "        word_counts[w]=0\n",
        "      word_counts[w]+=1\n",
        "\n",
        "    #generate list in this format:  (word, (doc_id, tf_value)) as required\n",
        "    tf_list=[]\n",
        "    for w,count in word_counts.items():\n",
        "      tf_value=count/total_words #relative word frequency\n",
        "      tf_list.append((w,(doc_id,tf_value)))\n",
        "\n",
        "\n",
        "    return tf_list  # York Code\n",
        "\n",
        "# Convert doc_words_rdd into tf_rdd whiose element is (word, (doc_id, tf_value))\n",
        "tf_rdd = doc_words_rdd.flatMap(compute_tf)\n",
        "\n",
        "#debug\n",
        "#tf_rdd.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJmH7bFRMqCO"
      },
      "source": [
        "**From tf_rdd, compute the document frequency of each word and put the results in a RDD (df_rdd), where each element is (word, doc_count)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcM4wfCxbmoT"
      },
      "outputs": [],
      "source": [
        "# Compute Document Frequency (DF) per word\n",
        "#simple\n",
        "#as long as we know each 'word' occured in which 'doc_id'\n",
        "#so first we retrive (word, doc_id)\n",
        "word_doc_rdd=tf_rdd.map(lambda x:(x[0],x[1][0]))\n",
        "\n",
        "#do distinct operation to (word,doc_id), to elimate duplicate\n",
        "#since in each single doc, one word may occur more than once, but df only need count once\n",
        "word_doc_distinct_rdd=word_doc_rdd.distinct()\n",
        "\n",
        "#conver (word, doc_id) to (word,1), then by reduceByKei get sum, we can get (word, doc_count)\n",
        "df_rdd=word_doc_distinct_rdd.map(lambda x: (x[0],1))\\\n",
        "                .reduceByKey(lambda a,b:a+b)\n",
        "\n",
        "#debug\n",
        "#df_rdd.take(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_FPPg9YS4VV"
      },
      "source": [
        "\n",
        "The idf value of a word is computed as $log\\frac{N}{df+1}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_-h2tWcbv1s"
      },
      "outputs": [],
      "source": [
        "# Compute Inverted Document Frequency (IDF)\n",
        "#since N is the total number of doc, we can retrive it from pervious doc_words_rdd\n",
        "N=doc_words_rdd.count()\n",
        "\n",
        "#apply formula: log(N/df+1)\n",
        "idf_rdd=df_rdd.mapValues(lambda df:math.log(N/(df+1)))\n",
        "\n",
        "#debug\n",
        "#idf_rdd.take(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeewFTmiUJX8"
      },
      "source": [
        "**From tf_rdd and idf_rdd, compute the tf-idf value of a word for each document and put the results in another RDD tfidf_rdd whose element is (word, (doc_id, tf-idf))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzE503R-cY2E"
      },
      "outputs": [],
      "source": [
        "# Compute TF-IDF\n",
        "#based on the pervious result\n",
        "#when join key(word), we should get something like\n",
        "#(word,((doc_idtfidf_value),idf_value)) - check debug code result\n",
        "joined_rdd=tf_rdd.join(idf_rdd)\n",
        "\n",
        "#use the rdd after joined, we can calculate tf*idf get tf-idf\n",
        "#output format should like: (word,(doc_id,tfidf_value))\n",
        "tfidf_rdd=joined_rdd.map(lambda x:(x[0],(x[1][0][0],x[1][0][1]*x[1][1])))\n",
        "\n",
        "#debug\n",
        "#tfidf_rdd.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH78_35Seo5A"
      },
      "source": [
        "**Convert tfidf_rdd to an inverted index RDD whose element is (word, [(doc_id, tf-idf), (doc_id, tf-idf), ...])**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp9oQHY7bxLu",
        "outputId": "2725a1a7-cd68-4add-b960-f49c1e4b63f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverted index collected as dictionary. Size = 5545\n"
          ]
        }
      ],
      "source": [
        "# Convert to Inverted Index Format\n",
        "#covert to interted index format\n",
        "inverted_index_rdd=tfidf_rdd.groupByKey().mapValues(list)\n",
        "\n",
        "#originally, I plan to use .lookup() function\n",
        "#it well knowed in excel, and work similer like that to seach value in column and row\n",
        "#i have no idea why colab report that was an error....\n",
        "\n",
        "#therefore, i just collect them into a python dictionary\n",
        "#further retrieve we can just use driver to get value\n",
        "#this is not bad i guess? since it even more faster?\n",
        "inverted_index_dict=inverted_index_rdd.collectAsMap()\n",
        "print(\"Inverted index collected as dictionary. Size =\",len(inverted_index_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0N0VVsELb8is",
        "outputId": "ff9e3ba3-5d14-461c-89fc-87c3cf24d001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "standard: [('941', 0.1311354067954801), ('1014', 0.041754557387615555), ('999', 0.19982538178358872), ('1003', 0.11418593244776498), ('1000', 0.059522454148303025), ('937', 0.07560960391811465), ('946', 0.028546483111941245), ('1001', 0.1613974237482832), ('993', 0.04823371284431452), ('940', 0.17484720906064014), ('1004', 0.17484720906064014), ('919', 0.06879234454844857), ('996', 0.09483238457526244), ('903', 0.030408210271415676), ('887', 0.09646742568862904), ('962', 0.04512186040274584), ('913', 0.06741097216795765), ('873', 0.06993888362425606), ('888', 0.1149680278754894), ('877', 0.09024372080549169), ('860', 0.06993888362425606), ('841', 0.0717321883325703), ('842', 0.04701773689025617), ('824', 0.0822810395579483), ('815', 0.08742360453032007), ('748', 0.06823305719439615), ('755', 0.035412092974306864), ('724', 0.037804801959057326), ('657', 0.04823371284431452), ('603', 0.06582483164635863), ('608', 0.029761227074151513), ('604', 0.028258134797679215), ('655', 0.023910729444190107), ('593', 0.02566564536669947), ('520', 0.043711802265160034), ('530', 0.03633188759701613), ('512', 0.016077904281438172), ('445', 0.03832267595849646), ('541', 0.05379914124942774), ('425', 0.11656480604042675), ('395', 0.058282403020213376), ('420', 0.04823371284431452), ('358', 0.027698567771982596), ('311', 0.031083948277447135), ('272', 0.043711802265160034), ('291', 0.027160731504565456), ('208', 0.028258134797679215), ('266', 0.07770987069361783), ('217', 0.03633188759701613), ('223', 0.01998253817835887), ('171', 0.04238720219651882), ('195', 0.03996507635671774), ('136', 0.022202820198176525), ('153', 0.029141201510106688), ('123', 0.01589520082369456), ('158', 0.07770987069361783), ('17', 0.034537720308274594), ('6', 0.02168647554240498), ('22', 0.029761227074151513), ('1435', 0.13011885325442987), ('1454', 0.043711802265160034), ('1436', 0.05278406311264608), ('1452', 0.04741619228763122), ('1412', 0.0932518448323414), ('1378', 0.08477440439303764), ('1391', 0.04114051977897415), ('1321', 0.035412092974306864), ('1359', 0.07993015271343548), ('1210', 0.05278406311264608), ('1272', 0.036809938749608445), ('1174', 0.08477440439303764), ('1216', 0.020570259889487075), ('1251', 0.038854935346808915), ('1165', 0.04823371284431452), ('1144', 0.02797555344970242), ('1215', 0.01998253817835887), ('1130', 0.043711802265160034), ('1099', 0.03291241582317932), ('1124', 0.017162916226811303), ('1061', 0.05180658046241189), ('1043', 0.16217712144755028), ('1058', 0.04823371284431452), ('1062', 0.04114051977897415), ('1024', 0.04054428036188757), ('1026', 0.10361316092482378), ('1005', 0.0466259224161707), ('950', 0.029761227074151513), ('1012', 0.21519656499771095)]\n",
            "controversi: [('941', 0.0726114747669863), ('935', 0.05957864596265543), ('920', 0.1056166905701619), ('912', 0.11915729192531085), ('795', 0.042634260413643335), ('259', 0.0815286734225811), ('176', 0.051067410825133226), ('126', 0.05192328921885054), ('91', 0.2212921135755773), ('17', 0.01912400981517335), ('20', 0.06114650506693583), ('1305', 0.1161783596271781), ('1210', 0.08768178085070044)]\n",
            "oper: [('941', 0.03075112063840884), ('955', 0.03784753309342627), ('984', 0.029374204788927848), ('938', 0.03514413787246724), ('993', 0.03393227104927872), ('1007', 0.04187386640123757), ('997', 0.0667142956223107), ('911', 0.02771932001208684), ('976', 0.04278416784474273), ('907', 0.06150224127681768), ('964', 0.022884554893699602), ('970', 0.028115310297973796), ('897', 0.037133428695437086), ('884', 0.06055605294948203), ('913', 0.023711707480218867), ('882', 0.07569506618685254), ('917', 0.03936143441716332), ('857', 0.03644577260848455), ('839', 0.0242971817389897), ('867', 0.05466865891272682), ('830', 0.03075112063840884), ('838', 0.03514413787246724), ('842', 0.033076835644675055), ('961', 0.0634861845438118), ('820', 0.011442277446849801), ('811', 0.018222886304242274), ('815', 0.09225336191522651), ('797', 0.07028827574493449), ('779', 0.05466865891272682), ('809', 0.04373492713018146), ('819', 0.022364451373388248), ('822', 0.07473690079208224), ('802', 0.02852277856316182), ('817', 0.03393227104927872), ('792', 0.028115310297973796), ('710', 0.10358272215042977), ('698', 0.06560239069527218), ('709', 0.060247093495658134), ('690', 0.03578312219742119), ('758', 0.05517958095864016), ('692', 0.01839319365288005), ('773', 0.05367468329613179), ('681', 0.07028827574493449), ('739', 0.03644577260848455), ('745', 0.10933731782545364), ('684', 0.13120478139054437), ('737', 0.05963853699570199), ('674', 0.058748409577855695), ('704', 0.06786454209855744), ('654', 0.03858964158545423), ('732', 0.13572908419711488), ('664', 0.013667164728181705), ('720', 0.037133428695437086), ('644', 0.02852277856316182), ('659', 0.03280119534763609), ('666', 0.016000583096407853), ('742', 0.05466865891272682), ('642', 0.03858964158545423), ('687', 0.0242971817389897), ('617', 0.10223749199263199), ('627', 0.020500747092272557), ('615', 0.0634861845438118), ('723', 0.04373492713018146), ('594', 0.016678573905577674), ('621', 0.03514413787246724), ('619', 0.02733432945636341), ('585', 0.05391977317419632), ('590', 0.024600896510727072), ('608', 0.020936933200618783), ('584', 0.10358272215042977), ('575', 0.01696613552463936), ('572', 0.01968071720858166), ('591', 0.034527574050143256), ('547', 0.025559372998157998), ('581', 0.02752547861340092), ('593', 0.05416711158325227), ('528', 0.04232412302920787), ('527', 0.03644577260848455), ('523', 0.047614638407858845), ('525', 0.0848306776231968), ('511', 0.018923766546713135), ('529', 0.025559372998157998), ('515', 0.025231688728950843), ('512', 0.01131075701642624), ('517', 0.028115310297973796), ('503', 0.049201793021454145), ('491', 0.10933731782545364), ('452', 0.037133428695437086), ('445', 0.02695988658709816), ('494', 0.08200298836909023), ('500', 0.03075112063840884), ('508', 0.041001494184545115), ('490', 0.05963853699570199), ('497', 0.06247846732883065), ('448', 0.019485858622358077), ('433', 0.03578312219742119), ('489', 0.01968071720858166), ('484', 0.04742341496043773), ('400', 0.015744573766865326), ('406', 0.02852277856316182), ('465', 0.04278416784474273), ('408', 0.0667142956223107), ('366', 0.03335714781115535), ('365', 0.032263470833740426), ('378', 0.02733432945636341), ('348', 0.01757206893623362), ('359', 0.01773037586358708), ('299', 0.07028827574493449), ('310', 0.019294820792727114), ('302', 0.057884462378181345), ('327', 0.019485858622358077), ('309', 0.10933731782545364), ('350', 0.05391977317419632), ('291', 0.038214984871032344), ('304', 0.037133428695437086), ('265', 0.034527574050143256), ('250', 0.031239233664415327), ('228', 0.021162061514603935), ('249', 0.037133428695437086), ('243', 0.029374204788927848), ('188', 0.014687102394463924), ('178', 0.044728902746776496), ('227', 0.034527574050143256), ('164', 0.049824600528054826), ('146', 0.16400597673818046), ('134', 0.037133428695437086), ('126', 0.03298444224901954), ('122', 0.032263470833740426), ('128', 0.05623062059594759), ('115', 0.0485943634779794), ('102', 0.028942231189090672), ('129', 0.014578309043393821), ('141', 0.0202893991841048), ('153', 0.020500747092272557), ('158', 0.018222886304242274), ('64', 0.02262151403285248), ('47', 0.016131735416870213), ('89', 0.022492248238379038), ('67', 0.051118745996315995), ('48', 0.07028827574493449), ('75', 0.08200298836909023), ('104', 0.02186746356509073), ('31', 0.07569506618685254), ('44', 0.028942231189090672), ('25', 0.050463377457901686), ('17', 0.008099060579663234), ('38', 0.049201793021454145), ('21', 0.02262151403285248), ('27', 0.10734936659226357), ('16', 0.015744573766865326), ('74', 0.05623062059594759), ('1431', 0.037133428695437086), ('1426', 0.022364451373388248), ('1445', 0.02695988658709816), ('1418', 0.006604267519658274), ('1407', 0.03280119534763609), ('1408', 0.021162061514603935), ('1412', 0.06560239069527218), ('1397', 0.10543241361740173), ('1358', 0.021162061514603935), ('1364', 0.0728915452169691), ('1368', 0.02400087464461178), ('1345', 0.04373492713018146), ('1362', 0.04232412302920787), ('1352', 0.051791361075214884), ('1317', 0.06150224127681768), ('1380', 0.015744573766865326), ('1328', 0.02733432945636341), ('1327', 0.03644577260848455), ('1318', 0.02733432945636341), ('1359', 0.05623062059594759), ('1274', 0.028115310297973796), ('1247', 0.04187386640123757), ('1248', 0.012779686499078999), ('1207', 0.03280119534763609), ('1208', 0.050463377457901686), ('1231', 0.016400597673818046), ('1227', 0.03393227104927872), ('1257', 0.05623062059594759), ('1192', 0.03784753309342627), ('1183', 0.08315796003626053), ('1193', 0.08200298836909023), ('1182', 0.0317430922719059), ('1159', 0.06786454209855744), ('1143', 0.07717928317090846), ('1175', 0.019107492435516172), ('1147', 0.013957955467079189), ('1171', 0.026595563795380618), ('1219', 0.08464824605841574), ('1091', 0.011442277446849801), ('1105', 0.02771932001208684), ('1070', 0.025559372998157998), ('1051', 0.0317430922719059), ('1022', 0.08556833568948546), ('1043', 0.02852277856316182), ('1035', 0.037133428695437086), ('1015', 0.04373492713018146), ('1038', 0.030278026474741013), ('960', 0.06786454209855744), ('1027', 0.032263470833740426), ('959', 0.0317430922719059), ('949', 0.020936933200618783), ('1009', 0.058748409577855695)]\n",
            "user: [('941', 0.05694874717739913), ('994', 0.028926347772647177), ('928', 0.024137217346712214), ('1013', 0.03719101856483208), ('984', 0.027199401636966748), ('993', 0.06283999688540594), ('991', 0.07009076575679893), ('942', 0.06394245297111481), ('918', 0.03254214124422807), ('976', 0.039616519775582004), ('963', 0.031971226485557405), ('894', 0.01292453836650193), ('892', 0.029392901768980195), ('962', 0.05878580353796039), ('913', 0.04391228698016319), ('879', 0.03573254724856416), ('855', 0.049252970531804656), ('870', 0.20248443440853023), ('845', 0.021439528349138495), ('839', 0.022498270489836693), ('818', 0.11389749435479826), ('820', 0.03178534726180417), ('840', 0.09345435434239857), ('813', 0.014578879277414178), ('826', 0.024626485265902328), ('806', 0.030372665161279535), ('807', 0.021439528349138495), ('828', 0.031971226485557405), ('779', 0.07593166290319883), ('793', 0.020946665628468645), ('781', 0.027199401636966748), ('783', 0.05062110860213256), ('771', 0.049252970531804656), ('801', 0.08677904331794153), ('701', 0.015985613242778703), ('799', 0.030372665161279535), ('695', 0.030372665161279535), ('692', 0.01703140102501656), ('773', 0.04970072480936651), ('676', 0.030372665161279535), ('731', 0.021190231507869445), ('730', 0.036447198193535445), ('732', 0.06283999688540594), ('724', 0.049252970531804656), ('720', 0.06876829847836875), ('721', 0.057852695545294354), ('718', 0.05694874717739913), ('728', 0.054670797290303164), ('648', 0.07810113898614739), ('659', 0.030372665161279535), ('637', 0.04613569391586765), ('743', 0.044447802675043224), ('726', 0.10719764174569248), ('629', 0.027199401636966748), ('665', 0.018787215563678062), ('719', 0.09111799548383862), ('636', 0.010124221720426513), ('615', 0.0881787053069406), ('723', 0.04049688688170605), ('625', 0.028474373588699566), ('631', 0.036447198193535445), ('634', 0.022223901337521612), ('626', 0.046727177171199286), ('624', 0.054670797290303164), ('594', 0.04633118414432472), ('612', 0.028926347772647177), ('607', 0.023067846957933826), ('630', 0.056361646691034196), ('660', 0.015313948820813212), ('614', 0.021439528349138495), ('608', 0.019386807549752895), ('584', 0.031971226485557405), ('575', 0.015709999221351485), ('610', 0.04992766875826773), ('580', 0.023978419864168054), ('582', 0.04795683972833611), ('591', 0.031971226485557405), ('547', 0.023667011813984055), ('606', 0.06074533032255907), ('533', 0.05694874717739913), ('534', 0.03254214124422807), ('593', 0.06687559301566137), ('528', 0.09797633922993398), ('523', 0.014696450884490097), ('538', 0.052822026367442675), ('529', 0.023667011813984055), ('451', 0.10719764174569248), ('512', 0.05236666407117162), ('458', 0.052822026367442675), ('456', 0.11949901047060801), ('514', 0.026411013183721337), ('491', 0.05062110860213256), ('492', 0.10266816392545196), ('445', 0.024963834379133864), ('495', 0.018982915725799708), ('508', 0.037965831451599416), ('490', 0.0552230275659628), ('497', 0.014463173886323588), ('433', 0.03313381653957768), ('486', 0.022779498870959654), ('502', 0.0854231207660987), ('510', 0.05062110860213256), ('546', 0.04795683972833611), ('429', 0.021190231507869445), ('484', 0.06586843047024478), ('482', 0.026411013183721337), ('405', 0.04555899774191931), ('403', 0.019808259887791002), ('382', 0.036447198193535445), ('475', 0.020708635337236048), ('378', 0.05062110860213256), ('358', 0.0180431674225423), ('373', 0.07289439638707089), ('461', 0.020475954041312045), ('348', 0.016271070622114036), ('370', 0.11389749435479826), ('334', 0.05062110860213256), ('342', 0.030627897641626423), ('310', 0.07146509449712832), ('280', 0.04238046301573889), ('295', 0.039616519775582004), ('327', 0.0180431674225423), ('298', 0.02566704098136299), ('309', 0.02531055430106628), ('274', 0.031971226485557405), ('277', 0.028926347772647177), ('210', 0.018982915725799708), ('286', 0.10124221720426511), ('250', 0.014463173886323588), ('252', 0.046727177171199286), ('257', 0.01859550928241604), ('207', 0.03538562931411208), ('258', 0.041417270674472095), ('208', 0.03681535171064187), ('204', 0.052067425990764915), ('234', 0.06177491219243295), ('191', 0.02679941043642312), ('243', 0.027199401636966748), ('202', 0.05097510236858104), ('190', 0.10354317668618024), ('217', 0.023667011813984055), ('223', 0.026033712995382458), ('220', 0.01703140102501656), ('179', 0.039616519775582004), ('212', 0.028036306302719572), ('222', 0.02679941043642312), ('213', 0.052067425990764915), ('154', 0.024963834379133864), ('175', 0.031971226485557405), ('218', 0.021694760829485384), ('224', 0.023067846957933826), ('167', 0.030887456096216476), ('139', 0.033747405734755036), ('163', 0.021694760829485384), ('126', 0.01018078161830599), ('124', 0.08962425785295601), ('145', 0.034384149239184376), ('161', 0.04338952165897077), ('147', 0.035045382878399464), ('122', 0.029874752617652003), ('131', 0.01656690826978884), ('132', 0.015443728048108238), ('135', 0.02566704098136299), ('115', 0.06749481146951007), ('137', 0.024963834379133864), ('90', 0.1051361486351984), ('123', 0.010354317668618024), ('70', 0.06508428248845614), ('52', 0.036447198193535445), ('47', 0.044812128926478005), ('66', 0.039616519775582004), ('67', 0.11833505906992026), ('56', 0.06508428248845614), ('46', 0.15186332580639766), ('34', 0.03141999844270297), ('28', 0.032254157693394195), ('42', 0.023978419864168054), ('16', 0.014578879277414178), ('10', 0.021439528349138495), ('49', 0.08889560535008645), ('13', 0.11389749435479826), ('2', 0.05062110860213256), ('9', 0.019386807549752895), ('1429', 0.039616519775582004), ('1436', 0.034384149239184376), ('1450', 0.0180431674225423), ('1440', 0.06177491219243295), ('1451', 0.037965831451599416), ('1404', 0.14696450884490098), ('1445', 0.04992766875826773), ('1418', 0.006115301710324739), ('1396', 0.014578879277414178), ('1442', 0.054398803273933496), ('1376', 0.044447802675043224), ('1375', 0.09266236828864945), ('1358', 0.0195952678459868), ('1361', 0.19808259887791002), ('1360', 0.027199401636966748), ('1368', 0.022223901337521612), ('1365', 0.031971226485557405), ('1363', 0.020946665628468645), ('1353', 0.049252970531804656), ('1352', 0.04795683972833611), ('1357', 0.03719101856483208), ('1367', 0.08962425785295601), ('1319', 0.07289439638707089), ('1327', 0.033747405734755036), ('1263', 0.07009076575679893), ('1258', 0.028036306302719572), ('1256', 0.048596264258047264), ('1236', 0.026411013183721337), ('1229', 0.0195952678459868), ('1211', 0.037965831451599416), ('1183', 0.02566704098136299), ('1174', 0.0276115137829814), ('1170', 0.03573254724856416), ('1195', 0.11157305569449626), ('1143', 0.03573254724856416), ('1215', 0.026033712995382458), ('1171', 0.024626485265902328), ('1118', 0.0180431674225423), ('1103', 0.04555899774191931), ('1107', 0.15186332580639766), ('1128', 0.035045382878399464), ('1113', 0.031971226485557405), ('1054', 0.02679941043642312), ('1042', 0.034384149239184376), ('1078', 0.06876829847836875), ('1043', 0.026411013183721337), ('1035', 0.06876829847836875), ('1058', 0.06283999688540594), ('1019', 0.03313381653957768), ('958', 0.020708635337236048), ('957', 0.019386807549752895), ('959', 0.05878580353796039)]\n",
            "lc: [('941', 0.0726114747669863), ('998', 0.08012300663943316), ('990', 0.08449335245612952), ('940', 0.09681529968931507), ('978', 0.0726114747669863), ('970', 0.06638763407267319), ('884', 0.07149437515518652), ('859', 0.37177075080696986), ('290', 0.11064605678778865), ('275', 0.16898670491225903), ('16', 0.03717707508069699), ('1230', 0.13405195341597473), ('1216', 0.03417010577269944)]\n",
            "intensifi: [('463', 0.13110883007961094), ('228', 0.06343975649013432), ('1094', 0.10727086097422711)]\n",
            "Sample doc IDs: ['941', '1014', '935', '1002', '1006', '1011', '999', '951', '998', '994']\n"
          ]
        }
      ],
      "source": [
        "# Print a sample of the TF-IDF based inverted index\n",
        "for word, postings in inverted_index_rdd.take(5):\n",
        "    print(f\"{word}: {postings}\")\n",
        "#---------------------------above is original code-------------------------------\n",
        "#----------all below code will affect speed, comment it as needed----------\n",
        "#-------------------------since they are all debug------------------------------\n",
        "#debug code by myself\n",
        "#read eclass a2 requirement page, I see sample inverted index\n",
        "#here i just want see the my output compare to sample\n",
        "#target_word=\"intensifi\" #other word for testing: adher, simplifi, intensifi\n",
        "#results=inverted_index_rdd.filter(lambda x:x[0]==target_word).collect()\n",
        "#if results:\n",
        "#  word,postings=results[0]\n",
        "#  print(f\"{word}: {postings}\")\n",
        "#else:\n",
        "#  print(f\"No results found for word: {target_word}\")\n",
        "\n",
        "#read the page:\n",
        "#The ID of a document should be the prefix of its file name. For example, the ID of document 3.txt is '3'.\n",
        "#here i wan't sort, as long as i got correct doc_id\n",
        "#sample_doc_ids = doc_words_rdd.map(lambda x: x[0]).take(10)\n",
        "#print(\"Sample doc IDs:\", sample_doc_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4I4UmqaYiXV",
        "outputId": "f43e3a28-532f-484b-c0b9-f36bd0eb3408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5545"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See how many words in the inverted index\n",
        "inverted_index_rdd.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKsPsi6y2vCA"
      },
      "source": [
        "Vector Space Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smd52xGeLtHc"
      },
      "source": [
        "\n",
        "\n",
        "**Query processing: define a function to convert a query into its tf-idf representation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdjjyvjh3Bfx"
      },
      "outputs": [],
      "source": [
        "# Put the word IDF values in a dictionary for fast lookup during query processing\n",
        "idf_lookup = idf_rdd.collectAsMap()\n",
        "\n",
        "# Function to process a query into a sparse TF-IDF dictionary\n",
        "def process_query(query):\n",
        "    # N it number of doc, is used to search words that have not appeared\n",
        "    N=doc_words_rdd.count()\n",
        "\n",
        "    #break word & lowercase, only keep letters\n",
        "    tokens=re.findall(r\"[a-z]+\",query.lower())\n",
        "\n",
        "    #remove stopword\n",
        "    #this implemented before, same logic, check part 2\n",
        "    filtered_words=[word for word in tokens if word not in stop_words]\n",
        "\n",
        "    #stemming\n",
        "    stemmed_tokens=[stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "    #check each word occured time\n",
        "    word_counts={}\n",
        "    for w in stemmed_tokens:\n",
        "      if w not in word_counts:\n",
        "        word_counts[w]=0\n",
        "      word_counts[w]+=1\n",
        "\n",
        "    #calculate word TF value(relative frequent =count/total)\n",
        "    total_terms=len(stemmed_tokens)\n",
        "    query_tfidf={}\n",
        "    for w, count in word_counts.items():\n",
        "      #TF\n",
        "      tf=count/total_terms if total_terms>0 else 0\n",
        "\n",
        "      #if this word wasn't occured inside the doc, then df=0=>idf=log(n/(0+1))=log(n=N)\n",
        "      idf=idf_lookup.get(w,math.log(N))\n",
        "\n",
        "      #finally we can got the word in the seach TF-IDF\n",
        "      query_tfidf[w]=tf*idf\n",
        "\n",
        "\n",
        "    return query_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3xSgJfN4UBB"
      },
      "source": [
        "**Precompute Document Norms:**\n",
        "The L2 norm of each document is computed once to speed up document score computation. This reduces redundant calculations at query time. This computation should be done based on the inverted index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inFZ9sno3pUJ",
        "outputId": "c48a6742-dea4-4003-fa11-2a4e1c7d20af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doc_id: 999, norm: 0.8315303035221504\n",
            "doc_id: 937, norm: 0.544077901108632\n",
            "doc_id: 946, norm: 0.45342040779059023\n",
            "doc_id: 996, norm: 0.6080724270867532\n",
            "doc_id: 913, norm: 0.5379992948346767\n"
          ]
        }
      ],
      "source": [
        "# Precompute document norms based on their words' tf-idf values and put the norms in a Python dictionary with key being doc_id and value being the document norm\n",
        "#from the pervious code result the inverted index format is:\n",
        "#standard: [('941', 0.10943756502450377), ('999', 0.12507150288514715), ('1003', 0.14293886044016818)\n",
        "#more standard way is: (word, [(doc_id1, tf_idf1),.......])\n",
        "\n",
        "#therefore we can extend each (word,posting) as (doc_id, tf_idf)\n",
        "doc_tf_idf_rdd=inverted_index_rdd.flatMap(lambda x:x[1])\n",
        "#!!here x[1] is posting list, not other things, had to remember this\n",
        "\n",
        "#calculate the sum of squares of all tf-idf values ​​for each doc_id\n",
        "#we can use reduceByKey to cumulate\n",
        "doc_tf_idf_sumsq_rdd=doc_tf_idf_rdd.map(lambda x:(x[0],x[1] ** 2)).reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#take square root of sum of square to get the L2 norm (this stand for each dic..)\n",
        "doc_norms_rdd=doc_tf_idf_sumsq_rdd.mapValues(math.sqrt)\n",
        "\n",
        "#collect result to py dictonary\n",
        "doc_norms=doc_norms_rdd.collectAsMap()\n",
        "\n",
        "#debug\n",
        "for d_id in list(doc_norms.keys())[:5]:\n",
        "  print(f\"doc_id: {d_id}, norm: {doc_norms[d_id]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYF7h4yTIN4Z"
      },
      "outputs": [],
      "source": [
        "# Function to compute document scores given a query using the vector space model\n",
        "def rank_documents(query, top_k):\n",
        "    #to see why use pyton dictionary, check code block near end of part II\n",
        "\n",
        "    #retrive tf-idf, we just done before\n",
        "    query_tfidf=process_query(query)\n",
        "\n",
        "    #scores is used to accumulate the dot product (q·d) for each document\n",
        "    scores={}\n",
        "\n",
        "    #(q⋅d) / ||d||\n",
        "    #(q⋅d) is for BIG PART: 1\n",
        "    #||d|| is for BIG PART 2\n",
        "\n",
        "#---------------------------------------------------\n",
        "    #BIG PART: 1 ------------ (q⋅d)\n",
        "#---------------------------------------------------\n",
        "    #for more information why use dictionary, check code block at the end of part 2\n",
        "    #iterate over each word q_word in the query vector\n",
        "    #core idea here is retrieve each doc contain this word\n",
        "    for q_word,q_tfidf_value in query_tfidf.items():\n",
        "      #posting format: [(doc_id, doc_tfidf),.........]\n",
        "      postings_list=inverted_index_dict.get(q_word, [])\n",
        "      # if posting list is empty | word is not occured, then skip\n",
        "      for (doc_id,doc_tfidf_value) in postings_list:\n",
        "        #dot_product contribute for current\n",
        "        score_contribtion=q_tfidf_value*doc_tfidf_value\n",
        "        scores[doc_id]=scores.get(doc_id,0.0)+score_contribtion\n",
        "\n",
        "#---------------------------------------------------\n",
        "    #BIG PART: 2 ------------ ||d||\n",
        "#---------------------------------------------------\n",
        "    #here divide ||d|| based on above score\n",
        "    final_scores=[]\n",
        "    for doc_id,dot_product in scores.items():\n",
        "      #if this doc is in doc_norms and range is not 0, then divide /\n",
        "      if doc_id in doc_norms and doc_norms[doc_id]!=0:\n",
        "        score=dot_product/doc_norms[doc_id]\n",
        "        final_scores.append((doc_id,score))\n",
        "\n",
        "    #finally, sort from high to low, and take front top\n",
        "    ranked_docs=sorted(final_scores,key=lambda x:x[1],reverse=True)[:top_k]\n",
        "\n",
        "\n",
        "    return ranked_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRKbT5hLml2V"
      },
      "source": [
        "**Given a query, call the rank_documents function and print the top-10 documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kP0xzGSKnJdm",
        "outputId": "dd75409e-f7bc-45d2-f3fb-f5c093c4e20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Relevant Documents:\n",
            "\n",
            "Doc_ID: Document Score\n",
            "377: 0.34316571310187943\n",
            "869: 0.30415914371393965\n",
            "824: 0.23371892909299502\n",
            "1144: 0.22246147894220142\n",
            "856: 0.2212078128278848\n",
            "1419: 0.21683483722162045\n",
            "715: 0.2066610196302842\n",
            "812: 0.20652041350135483\n",
            "489: 0.19080310866870978\n",
            "194: 0.18652349192687342\n"
          ]
        }
      ],
      "source": [
        "# Example Query 1\n",
        "query = \"Specific advantages of computerized index systems.\"\n",
        "top_docs = rank_documents(query, 10)\n",
        "\n",
        "# Print top 10 relevant documents\n",
        "print(\"\\nTop 10 Relevant Documents:\")\n",
        "print(\"\\nDoc_ID: Document Score\")\n",
        "for doc, score in top_docs:\n",
        "    print(f\"{doc}: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buL3WXHSsT5e",
        "outputId": "225ce3c7-4b22-49a7-8fa9-02866a4ec313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Relevant Documents:\n",
            "\n",
            "Doc_ID: Document Score\n",
            "1138: 0.2698882732022883\n",
            "532: 0.26532484401971396\n",
            "1155: 0.1958742695858231\n",
            "562: 0.15561758487265828\n",
            "309: 0.15488168234340965\n",
            "790: 0.1425356353084291\n",
            "1327: 0.14038899967739984\n",
            "451: 0.13858007618172363\n",
            "315: 0.13833282930878682\n",
            "58: 0.13745453463399604\n"
          ]
        }
      ],
      "source": [
        "# Example Query 2\n",
        "query = \"How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\"\n",
        "top_docs = rank_documents(query, 10)\n",
        "\n",
        "# Print top 10 relevant documents\n",
        "print(\"\\nTop 10 Relevant Documents:\")\n",
        "print(\"\\nDoc_ID: Document Score\")\n",
        "for doc, score in top_docs:\n",
        "    print(f\"{doc}: {score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
